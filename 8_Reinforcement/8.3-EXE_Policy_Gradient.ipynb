{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "8.3-EXE_Policy_Gradient.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ts5TvDxzB8"
      },
      "source": [
        "# Solve cartpole with REINFORCE\n",
        "\n",
        "> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))\n",
        "\n",
        "**2019 update:** Changes have been made to the display of environments due to the previous `viewer` being incompatible with newer versions of Gym.\n",
        "\n",
        "In this part, we will create an agent that can learn to solve the [cartpole problem](https://gym.openai.com/envs/CartPole-v0/) from OpenAI Gym by applying a simple policy gradient method called REINFORCE.\n",
        "In the cartpole problem, we need to balance a pole on a cart that moves along a track by applying left and right forces to the cart.\n",
        "\n",
        "We will implement a probabilistic policy, that given a state of the environment, $s$, outputs a probability distribution over available actions, $a$:\n",
        "\n",
        "$$\n",
        "p_\\theta(a|s)\n",
        "$$\n",
        "\n",
        "The policy is a neural network with parameters $\\theta$ that can be trained with gradient descent.\n",
        "When the set of available actions is discrete, we can use a network with softmax output do describe the distribution.\n",
        "The core idea of training the policy network is quite simple: *we want to maximize the expected total reward by increasing the probability of good actions and decreasing the probability of bad actions*. \n",
        "\n",
        "To achieve this, we apply the gradient of the expected discounted total reward (return):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] &= \\nabla_\\theta \\int p_\\theta(a|s) R({a}) \\, da \\\\\n",
        "&= \\int \\nabla_\\theta p_\\theta(a|s) R(a)  \\, da \\\\\n",
        "&= \\int p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) R(a) \\, da \\\\\n",
        "&= \\mathbb{E}[\\nabla_\\theta \\log p_\\theta(a|s) R(a)]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "by definition of expectation and using the identity \n",
        "\n",
        "$$\n",
        "\\nabla_\\theta p_\\theta(a|s) = p_\\theta(a|s) \\nabla_\\theta \\log p_\\theta(a|s) \\ .\n",
        "$$\n",
        "\n",
        "The expectation cannot be evaluated analytically, but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return a sequence of *actions*, *states* and *rewards*. This allows us to replace the integral with a Monte Carlo average:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla_\\theta \\log p_\\theta(a_t|s_t) R_t \\ ,\n",
        "$$\n",
        "\n",
        "which is our final gradient estimator, also known as REINFORCE. In the Monte Carlo estimator we run the environment simulator for a predefined number of steps with actions chosen stochastically according to the current stochastic action network $p_\\theta(a|s)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjbk48shxzCF"
      },
      "source": [
        "*Note: For simple reinforcement learning problems (like the one we will address in this exercise) there are simpler methods that work just fine. However, the Policy Gradient method (with some extensions) has been shown to also work well for complex problems with high dimensional inputs and many parameters, where simple methods become inadequate.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkpxtpJxzCG"
      },
      "source": [
        "## Policy gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78g14W2zxzCH"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from gym import wrappers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1zUzWA4xzCJ"
      },
      "source": [
        "First we create the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKJIPWNjxzCK"
      },
      "source": [
        "env = gym.make('CartPole-v0') # Create environment"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIWbq5PFxzCK"
      },
      "source": [
        "A state in this environment is four numbers describing the position of the cart along with the angle and speed of the pole.\n",
        "There are two available actions: push the cart *left* or *right* encoded as 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei_FFOkGxzCL",
        "outputId": "6bd276bb-7cec-4007-d6b8-af058a709dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s = env.reset()\n",
        "a = env.action_space.sample()\n",
        "print('sample state:', s)\n",
        "print('sample action:', a )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample state: [-0.00632468  0.00390003 -0.02033392 -0.02557691]\n",
            "sample action: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McutSlwvxzCM"
      },
      "source": [
        "**Note:** you will likely not be able to render environments in a Google Colab instance. Therefore, it may be beneficial for you to run this week's notebooks locally and/or team up with another student if you do not have a local environment.\n",
        "\n",
        "The function below is used for display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgT1Fy3zxzCN"
      },
      "source": [
        "def show_replay():\n",
        "    \"\"\"\n",
        "    Not-so-elegant way to display the MP4 file generated by the Monitor wrapper inside a notebook.\n",
        "    The Monitor wrapper dumps the replay to a local file that we then display as a HTML video object.\n",
        "    \"\"\"\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % env.file_infix, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return HTML(data='''\n",
        "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
        "    .format(encoded.decode('ascii')))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgYdRKH-xzCO"
      },
      "source": [
        "Let us see how the environment looks when we just take random actions. Note that the episode ends when the pole either 1) is more than 15 degrees from vertical, 2) more outside of the frame or 3) the pole is successfully balanced for some fixed duration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJquYMoxzCP",
        "outputId": "b9b04c87-d2e9-49b7-d166-0b57eb726e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "env = gym.make('CartPole-v0') # Create environment\n",
        "env = wrappers.Monitor(env, \"./gym-results\", force=True) # Create wrapper to display environment\n",
        "env.reset() # Reset environment\n",
        "\n",
        "# Run environment\n",
        "while True:\n",
        "    env.render() # Render environment\n",
        "    action = env.action_space.sample() # Get a random action\n",
        "    _, _, done, _ = env.step(action) # Take a step\n",
        "    if done: break # Break if environment is done\n",
        "\n",
        "env.close() # Close environment\n",
        "show_replay()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NoSuchDisplayException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2c6a9ee19ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./gym-results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create wrapper to display environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Reset environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMmbmWxexzCQ"
      },
      "source": [
        "Taking random actions does not do a very good job at balancing the pole. Let us now apply the Policy Gradient method described above to solve this task!\n",
        "\n",
        "Let's first define our network and helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_cNyi2wxzCQ"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    \"\"\"Policy network\"\"\"\n",
        "\n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        # network\n",
        "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
        "        self.out = nn.Linear(n_hidden, n_outputs)\n",
        "        # training\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.out(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def loss(self, action_probabilities, returns):\n",
        "        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRhcJGQXxzCR"
      },
      "source": [
        "def compute_returns(rewards, discount_factor):\n",
        "    \"\"\"Compute discounted returns.\"\"\"\n",
        "    returns = np.zeros(len(rewards))\n",
        "    returns[-1] = rewards[-1]\n",
        "    for t in reversed(range(len(rewards)-1)):\n",
        "        returns[t] = rewards[t] + discount_factor * returns[t+1]\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2M2XtBxzCR"
      },
      "source": [
        "To start with, our policy will be a rather simple neural network with one hidden layer. We can retrieve the shape of the state space (input) and action space (output) from the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxEJE38xzCT"
      },
      "source": [
        "n_inputs = env.observation_space.shape[0]\n",
        "n_hidden = 20\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "print('state shape:', n_inputs)\n",
        "print('action shape:', n_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhaANED-xzCU"
      },
      "source": [
        "# training settings\n",
        "\n",
        "num_episodes = 800\n",
        "rollout_limit = 500 # max rollout length\n",
        "discount_factor = 1.0 # reward discount factor (gamma), 1.0 = no discount\n",
        "learning_rate = 0.001 # you know this by now\n",
        "val_freq = 100 # validation frequency\n",
        "\n",
        "# setup policy network\n",
        "\n",
        "policy = PolicyNet(n_inputs, n_hidden, n_outputs, learning_rate)\n",
        "\n",
        "# train policy network\n",
        "\n",
        "try:\n",
        "    training_rewards, losses = [], []\n",
        "    print('start training')\n",
        "    for i in range(num_episodes):\n",
        "        rollout = []\n",
        "        s = env.reset()\n",
        "        for j in range(rollout_limit):\n",
        "            # generate rollout by iteratively evaluating the current policy on the environment\n",
        "            with torch.no_grad():\n",
        "                a_prob = policy(torch.from_numpy(np.atleast_2d(s)).float())\n",
        "                a = torch.multinomial(a_prob, num_samples=1).squeeze().numpy()\n",
        "            s1, r, done, _ = env.step(a)\n",
        "            rollout.append((s, a, r))\n",
        "            s = s1\n",
        "            if done: break\n",
        "        # prepare batch\n",
        "        rollout = np.array(rollout)\n",
        "        states = np.vstack(rollout[:,0])\n",
        "        actions = np.vstack(rollout[:,1])\n",
        "        rewards = np.array(rollout[:,2], dtype=float)\n",
        "        returns = compute_returns(rewards, discount_factor)\n",
        "        # policy gradient update\n",
        "        policy.optimizer.zero_grad()\n",
        "        a_probs = policy(torch.from_numpy(states).float()).gather(1, torch.from_numpy(actions)).view(-1)\n",
        "        loss = policy.loss(a_probs, torch.from_numpy(returns).float())\n",
        "        loss.backward()\n",
        "        policy.optimizer.step()\n",
        "        # bookkeeping\n",
        "        training_rewards.append(sum(rewards))\n",
        "        losses.append(loss.item())\n",
        "        # print\n",
        "        if (i+1) % val_freq == 0:\n",
        "            # validation\n",
        "            validation_rewards = []\n",
        "            for _ in range(10):\n",
        "                s = env.reset()\n",
        "                reward = 0\n",
        "                for _ in range(rollout_limit):\n",
        "                    with torch.no_grad():\n",
        "                        a_prob = policy(torch.from_numpy(np.atleast_2d(s)).float())\n",
        "                        a = a_prob.argmax().item()\n",
        "                    s, r, done, _ = env.step(a)\n",
        "                    reward += r\n",
        "                    if done: break\n",
        "                validation_rewards.append(reward)\n",
        "            print('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}, mean loss: {:7.4f}'.format(i+1, np.mean(training_rewards[-val_freq:]), np.mean(validation_rewards), np.mean(losses[-val_freq:])))\n",
        "    print('done')\n",
        "except KeyboardInterrupt:\n",
        "    print('interrupt')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3nXd-wnxzCW"
      },
      "source": [
        "# plot results\n",
        "def moving_average(a, n=10) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret / n\n",
        "\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(211)\n",
        "plt.plot(range(1, len(training_rewards)+1), training_rewards, label='training reward')\n",
        "plt.plot(moving_average(training_rewards))\n",
        "plt.xlabel('episode'); plt.ylabel('reward')\n",
        "plt.xlim((0, len(training_rewards)))\n",
        "plt.legend(loc=4); plt.grid()\n",
        "plt.subplot(212)\n",
        "plt.plot(range(1, len(losses)+1), losses, label='loss')\n",
        "plt.plot(moving_average(losses))\n",
        "plt.xlabel('episode'); plt.ylabel('loss')\n",
        "plt.xlim((0, len(losses)))\n",
        "plt.legend(loc=4); plt.grid()\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEFLRgPoxzCX"
      },
      "source": [
        "Now let's review the solution!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ7NNn5-xzCX"
      },
      "source": [
        "env = wrappers.Monitor(env, \"./gym-results\", force=True) # Create wrapper to display environment\n",
        "s = env.reset()\n",
        "\n",
        "for _ in range(500):\n",
        "    env.render()\n",
        "    a = policy(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()\n",
        "    s, r, done, _ = env.step(a)\n",
        "    if done: break\n",
        "    \n",
        "env.close()\n",
        "show_replay()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUW7Gx1_xzCY"
      },
      "source": [
        "## Reducing variance\n",
        "\n",
        "By default, this gradient estimator has high variance and therefore variance reduction becomes important to learn more complex tasks.\n",
        "We can reduce variance by subtracting a baseline from the returns, which is unbiased in expectation:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{T} \\sum_{t=0}^T \\nabla_\\theta \\log p_\\theta(a_t|s_t) (R_t-b_t) \\ ,\n",
        "$$\n",
        "\n",
        "where the baseline, $b_t$, is estimated by the return a timestep $t$ averaged over $V$ rollouts.\n",
        "\n",
        "$$\n",
        "b_t = \\frac{1}{V} \\sum_{v=1}^V R_t^{(v)} \\ .\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTdlfmpbxzCY"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Now it is your turn! Make sure you read and understand the code, then play around with it and try to make it learn better and faster.\n",
        "\n",
        "Experiment with the:\n",
        "\n",
        "* number of episodes\n",
        "* discount factor\n",
        "* learning rate\n",
        "* network layers\n",
        "\n",
        "\n",
        "### Exercise 1 \n",
        "\n",
        "*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "*Answer here...*\n",
        "\n",
        "### Exercise 2 \n",
        "\n",
        "*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps:*\n",
        "\n",
        "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
        "\n",
        "* *What is the total reward?*\n",
        "* *What is the total future reward in each timestep?*\n",
        "* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
        "\n",
        "*Hint: See introdution notebook.*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "*Answer here...*\n",
        "\n",
        "*What is the total reward?*\n",
        "$$\n",
        "%R_t = r_t + r_{t+1} + r_{t+2} + \\dots + r_T \\ .\n",
        "R_t = r_t + r_{t+1} + r_{t+2} + \\dots + r_T = \\sum_{k=0}^{T-t} r_{t+k} \\ .\n",
        "$$\n",
        "\n",
        "$$\n",
        "R_t = 0 + 1 + 1 + 1 + 0 + 1 + 1 + 0 + 0 + 0 = 5\n",
        "$$\n",
        "\n",
        "*What is the total future reward in each timestep?*\n",
        "\n",
        "So for timestep *t*, the future reward is the sum of all the rewards from *t* til *T*. \n",
        "\n",
        "- > check code cell below\n",
        "\n",
        "*What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
        "\n",
        "- > check code cell below\n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "*In the training output, you will sometimes observe the validation reward starts out lower than the training reward but as training progresses they cross over and the validation reward becomes higher than the training reward. How can you explain this behavior?*\n",
        "\n",
        "*Hint: Do we use the policy network in the same way during training and validation?*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "*Answer here...*\n",
        "\n",
        "The validation reward starts out at 0.\n",
        "It is calculated by selecting the policy which maximizes the expected return from all states, according to the equation below.\n",
        "\n",
        "$$\n",
        "\\DeclareMathOperator*{\\argmax}{argmax}\n",
        "$$\n",
        "$$\n",
        "\\pi^* = \\argmax_\\pi \\mathbb{E}[R|\\pi] \\ .\n",
        "$$\n",
        "\n",
        " \n",
        "\n",
        "### Exercise 4\n",
        "\n",
        "*How does the policy gradient method we have used address the exploration-exploitation dilemma?*\n",
        "\n",
        "*Hint: See the introduction notebook about exploration-exploitation.*\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "*Answer here...*\n",
        "\n",
        "In the exploration-exploitation dilemma, the problem is when should the agent start exploiting, considering what it has learned in the exploring. Two strategies were presented, the epsilon-greedy and the softmax action selection. Both of them, use policy gradients to sample actions. \"We have a stochastic policy that samples actions and then actions that happen to eventually lead to good outcomes get encouraged in the future, and actions taken that lead to bad outcomes get discouraged.\" (http://karpathy.github.io/2016/05/31/rl/)\n",
        "\n",
        "### Exercise 5 [optional]\n",
        "\n",
        "Extend the code above to reduce variance of the gradient estimator by computing and subtracting the baseline estimate. \n",
        "\n",
        "*Hint: You need to sample a batch of rollouts (now we sample just one) for each update in order to compute the baseline, $b_t$.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtK-qMx-AtYV",
        "outputId": "3549bcc1-8459-4891-824b-4e1da89a373a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Exercise 2\n",
        "\n",
        "#What is the total future reward in each timestep?\n",
        "rewards= [0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
        "\n",
        "for i in range(len(rewards)):\n",
        "  print(\"r%d = %d\" % (i,sum(rewards[i:])))\n",
        "\n",
        "\n",
        "#*What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
        "\n",
        "gamma = 0.9\n",
        "for i in range(len(rewards)):\n",
        "  rewards[i] *= gamma**i\n",
        "  \n",
        "print(\"discounted future reward %f\" % sum(rewards))  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r0 = 5\n",
            "r1 = 5\n",
            "r2 = 4\n",
            "r3 = 3\n",
            "r4 = 2\n",
            "r5 = 2\n",
            "r6 = 1\n",
            "r7 = 0\n",
            "r8 = 0\n",
            "r9 = 0\n",
            "discounted future reward 3.560931\n"
          ]
        }
      ]
    }
  ]
}